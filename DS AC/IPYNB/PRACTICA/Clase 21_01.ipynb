{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprender las métricas de clasificación de ciencia de datos en Sci-kit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un área importante del modelado predictivo en ciencia de datos es la clasificación. La clasificación consiste en tratar de predecir de qué clase proviene una muestra particular de una población. Por ejemplo, si estamos tratando de predecir si un paciente en particular será re-hospitalizado, las dos clases posibles son hospitalizado (positivo) y no hospitalizado (negativo). Luego, el modelo de clasificación intenta predecir si cada paciente será hospitalizado o no. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/classification.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida que entrene su modelo predictivo de clasificación, deseará evaluar qué tan bueno es. Curiosamente, hay muchas formas diferentes de evaluar el rendimiento. La mayoría de los científicos de datos que usan Python para el modelado predictivo usan el paquete Python llamado scikit-learn. Scikit-learn contiene muchas funciones integradas para analizar el rendimiento de los modelos.\n",
    "En este tutorial, analizaremos algunas de estas métricas y escribiremos nuestras propias funciones desde cero para comprender las matemáticas detrás de algunas de ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tutorial cubrirá las siguientes funciones de métricas de `sklearn.metrics` :\n",
    "\n",
    "    - confusion_matrix\n",
    "    \n",
    "    - accuracy_score\n",
    "    \n",
    "    - recall_score\n",
    "    \n",
    "    - precision_score\n",
    "    \n",
    "    - f1_score\n",
    "    \n",
    "    - roc_curve\n",
    "    \n",
    "    - roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí escribiremos nuestras propias funciones desde cero asumiendo una clasificación de dos clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos un conjunto de datos de muestra que tenga las etiquetas reales (real_label) y las probabilidades de predicción para dos modelos (model_RF y model_LR). Aquí las probabilidades son la probabilidad de ser de clase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>model_RF</th>\n",
       "      <th>model_LR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.639816</td>\n",
       "      <td>0.531904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.490993</td>\n",
       "      <td>0.414496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.623815</td>\n",
       "      <td>0.569883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.506616</td>\n",
       "      <td>0.443674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.418302</td>\n",
       "      <td>0.369532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual_label  model_RF  model_LR\n",
       "0             1  0.639816  0.531904\n",
       "1             0  0.490993  0.414496\n",
       "2             1  0.623815  0.569883\n",
       "3             1  0.506616  0.443674\n",
       "4             0  0.418302  0.369532"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Datasets/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la mayoría de los proyectos de ciencia de datos, definirá un umbral para definir qué probabilidades de predicción se etiquetan como predichas positivas versus predichas negativas. Por ahora supongamos que el umbral es 0.5. Agreguemos dos columnas adicionales que convierten las probabilidades en etiquetas predichas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>model_RF</th>\n",
       "      <th>model_LR</th>\n",
       "      <th>predicted_RF</th>\n",
       "      <th>predicted_LR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.639816</td>\n",
       "      <td>0.531904</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.490993</td>\n",
       "      <td>0.414496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.623815</td>\n",
       "      <td>0.569883</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.506616</td>\n",
       "      <td>0.443674</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.418302</td>\n",
       "      <td>0.369532</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual_label  model_RF  model_LR  predicted_RF  predicted_LR\n",
       "0             1  0.639816  0.531904             1             1\n",
       "1             0  0.490993  0.414496             0             0\n",
       "2             1  0.623815  0.569883             1             1\n",
       "3             1  0.506616  0.443674             1             0\n",
       "4             0  0.418302  0.369532             0             0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh = 0.5\n",
    "df['predicted_RF'] = (df.model_RF >= 0.5).astype('int')\n",
    "df['predicted_LR'] = (df.model_LR >= 0.5).astype('int')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada una etiqueta real y una etiqueta predicha, lo primero que podemos hacer es dividir nuestras muestras en 4 cubos:\n",
    "    - True positive - actual = 1, predicted = 1\n",
    "    - False positive - actual = 1, predicted = 0\n",
    "    - False negative - actual = 0, predicted = 1\n",
    "    - True negative - actual = 0, predicted = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se pueden representar con la siguiente imagen (fuente original https://en.wikipedia.org/wiki/Precision_and_recall#/media/File:Precisionrecall.svg) y haremos referencia a esta imagen en muchos de los cálculos a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/buckets.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O podemos representarlos con la matriz de confusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/conf_matrix.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener la matriz de confusión (como una matriz de 2x2) de scikit learn, que toma como entradas las etiquetas reales y las etiquetas predichas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['actual_label', 'model_RF', 'model_LR', 'predicted_RF', 'predicted_LR'], dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15758,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.predicted_LR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5519, 2360],\n",
       "       [2832, 5047]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_confusion_matrix(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde había 5047 positivos verdaderos, 2360 falsos positivos, 2832 falsos negativos y 5519 verdaderos negativos. Definamos nuestras propias funciones para verificar `confusion_matrix`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 5047\n",
      "FN: 2832\n",
      "FP: 2360\n",
      "TN: 5519\n"
     ]
    }
   ],
   "source": [
    "def find_TP(y_true, y_pred):\n",
    "    # counts the number of true positives (y_true = 1, y_pred = 1)\n",
    "    return sum((y_true == 1) & (y_pred == 1))\n",
    "def find_FN(y_true, y_pred):\n",
    "    # counts the number of false negatives (y_true = 1, y_pred = 0)\n",
    "    return sum((y_true == 1) & (y_pred == 0))# your code here\n",
    "def find_FP(y_true, y_pred):\n",
    "    # counts the number of false positives (y_true = 0, y_pred = 1)\n",
    "    return sum((y_true == 0) & (y_pred == 1))# your code here\n",
    "def find_TN(y_true, y_pred):\n",
    "    # counts the number of true negatives (y_true = 0, y_pred = 0)\n",
    "    return sum((y_true == 0) & (y_pred == 0))# your code here\n",
    "\n",
    "print('TP:',find_TP(df.actual_label.values, df.predicted_RF.values))\n",
    "print('FN:',find_FN(df.actual_label.values, df.predicted_RF.values))\n",
    "print('FP:',find_FP(df.actual_label.values, df.predicted_RF.values))\n",
    "print('TN:',find_TN(df.actual_label.values, df.predicted_RF.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escribamos una función que calcule los cuatro para nosotros, y otra función para duplicar `confusion_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def find_conf_matrix_values(y_true,y_pred):\n",
    "    # calculate TP, FN, FP, TN\n",
    "    TP = find_TP(y_true,y_pred)\n",
    "    FN = find_FN(y_true,y_pred)\n",
    "    FP = find_FP(y_true,y_pred)\n",
    "    TN = find_TN(y_true,y_pred)\n",
    "    return TP,FN,FP,TN\n",
    "def my_confusion_matrix(y_true, y_pred):\n",
    "    TP,FN,FP,TN = find_conf_matrix_values(y_true,y_pred)\n",
    "    return np.array([[TN,FP],[FN,TP]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "confusion = my_confusion_matrix(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifiquemos que nuestras funciones hayan funcionado con las funciones incorporadas `array 'de Python y` array_equal` de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "assert  np.array_equal(my_confusion_matrix(df.actual_label.values, df.predicted_RF.values),\\\n",
    "                       confusion_matrix(df.actual_label.values, df.predicted_RF.values) ), 'my_confusion_matrix() is not correct for RF'\n",
    "\n",
    "assert  np.array_equal(my_confusion_matrix(df.actual_label.values, df.predicted_LR.values),\\\n",
    "                       confusion_matrix(df.actual_label.values, df.predicted_LR.values) ), 'my_confusion_matrix() is not correct for LR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados estos cuatro segmentos (TP, FP, FN, TN), podemos calcular muchas otras métricas de rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La métrica más común para la clasificación es la precisión, que es la fracción de muestras predichas correctamente como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/accuracy.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener la puntuación de precisión de scikit learn, que toma como entradas las etiquetas reales y las etiquetas predichas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5047 5519 2360 2832\n"
     ]
    }
   ],
   "source": [
    "#calcular el accuracy\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print(TP, TN, FP, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32948343698438887\n",
      "0.32948343698438887\n"
     ]
    }
   ],
   "source": [
    "print((FP + FN) / float(TP + TN + FP + FN))\n",
    "print(1 - metrics.accuracy_score(df.actual_label.values, df.predicted_RF.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#caclular el accuracy para los dos modelos (RF y LR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando accuracy como una métrica de rendimiento, el modelo RF es más preciso que el modelo LR. Entonces, ¿deberíamos parar aquí y decir que el modelo RF es el mejor modelo? ¡No! Accuracy no siempre es la mejor métrica para evaluar los modelos de clasificación. }\n",
    "\n",
    "Por ejemplo, digamos que estamos tratando de predecir algo que solo ocurre 1 de cada 100 veces. Podríamos construir un modelo que obtenga un 99% de accuracy al decir que el evento nunca sucedió. Sin embargo, capturamos el 0% de los eventos que nos interesan. La medida del 0% aquí es otra métrica de rendimiento conocida como recuperación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall (también conocida como sensibilidad) es la fracción de eventos positivos que predijo correctamente como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/recall.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener el puntaje de precisión de scikit-learn, que toma como entradas las etiquetas reales y las etiquetas predichas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# calcular el recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular el recall para los dos modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un método para aumentar el recall es aumentar la cantidad de muestras que define como positivas predichas al reducir el umbral para las predicciones positivas. Desafortunadamente, esto también aumentará el número de falsos positivos. Otra métrica de rendimiento llamada precisión tiene esto en cuenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La precisión es la fracción de eventos positivos pronosticados que en realidad son positivos como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/precision.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener el puntaje de precisión de scikit-learn, que toma como entradas las etiquetas reales y las etiquetas predichas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# calcular el recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular la precisión para los dos modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, parece que el modelo RF es mejor tanto en el recall como en la precisión. Pero, ¿qué haría si un modelo fuera mejor en el recall y el otro fuera mejor en la precisión? Un método que utilizan algunos científicos de datos se llama la puntuación F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El f1-score es la media armónica de recuerdo y precisión, con una puntuación más alta como mejor modelo. La puntuación de f1 se calcula utilizando la siguiente fórmula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/f1_score.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener el f1-score de scikit-learn, que toma como entradas las etiquetas reales y las etiquetas predichas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#calcular el f1-score para los dos modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 RF: %.3f'%(f1_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('F1 LR: %.3f'%(f1_score(df.actual_label.values, df.predicted_LR.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora, hemos asumido que definimos un umbral de 0.5 para seleccionar qué muestras se predicen como positivas. Si cambiamos este umbral, las métricas de rendimiento cambiarán. Como se muestra abajo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('scores with threshold = 0.5')\n",
    "print('Accuracy RF: %.3f'%(accuracy_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('Recall RF: %.3f'%(recall_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('Precision RF: %.3f'%(precision_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('F1 RF: %.3f'%(f1_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print(' ')\n",
    "print('scores with threshold = 0.25')\n",
    "print('Accuracy RF: %.3f'%(accuracy_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values)))\n",
    "print('Recall RF: %.3f'%(recall_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values)))\n",
    "print('Precision RF: %.3f'%(precision_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values)))\n",
    "print('F1 RF: %.3f'%(f1_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo evaluamos un modelo si no hemos elegido un umbral? Un método muy común es utilizar la curva ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# roc_curve and roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laa curvas ROC son MUY útiles para comprender el equilibrio entre la tasa de verdadero positivo y las tasas de falso positivo. Sci-kit learn ha incorporado funciones para curvas ROC y para analizarlas. Las entradas a estas funciones (`roc_curve` y` roc_auc_score`) son las etiquetas reales y las probabilidades predichas (no las etiquetas predichas). Tanto `roc_curve` como` roc_auc_score` son funciones complicadas, por lo que no tendremos que escribir estas funciones desde cero. En cambio, le mostraremos cómo usar las funciones de aprendizaje de sci-kit y le explicaremos los puntos clave. Comencemos usando `roc_curve` para hacer el diagrama ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_RF, tpr_RF, thresholds_RF = roc_curve(df.actual_label.values, df.model_RF.values)\n",
    "fpr_LR, tpr_LR, thresholds_LR = roc_curve(df.actual_label.values, df.model_LR.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `roc_curve` devuelve tres listas:\n",
    "\n",
    "    - thresholds = all unique prediction probabilities in descending order\n",
    "    \n",
    "    - fpr = the false positive rate (FP / (FP+TN)) for each threshold\n",
    "    \n",
    "    - tpr = the true positive rate  (TP / (TP+FN)) (i.e. recall) for each threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "thresholds_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fpr_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tpr_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos trazar la curva ROC para cada modelo como se muestra a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(fpr_RF, tpr_RF,'r-',label = 'RF')\n",
    "plt.plot(fpr_LR,tpr_LR,'b-', label= 'LR')\n",
    "plt.plot([0,1],[0,1],'k-',label='random')\n",
    "plt.plot([0,0,1,1],[0,1,1,1],'g-',label='perfect')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay un par de cosas que podemos observar de esta figura.\n",
    "\n",
    "    - Un modelo que adivina aleatoriamente la etiqueta dará como resultado la línea negra y desea tener un modelo que tenga una curva sobre esta línea negra.\n",
    "    \n",
    "    - Un ROC que está más lejos de la línea negra es mejor, por lo que RF (rojo) se ve mejor que LR (azul).\n",
    "    \n",
    "    - Aunque no se ve directamente, un umbral alto da como resultado un punto en la parte superior derecha y un umbral bajo da como resultado un punto en la parte inferior izquierda. Esto significa que a medida que aumenta el umbral, obtiene un TPR más alto a un costo de FPR más alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para analizar el rendimiento, utilizaremos la métrica de área bajo curva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_RF = roc_auc_score(df.actual_label.values, df.model_RF.values)\n",
    "auc_LR = roc_auc_score(df.actual_label.values, df.model_LR.values)\n",
    "\n",
    "print('AUC RF:%.3f'% auc_RF)\n",
    "print('AUC LR:%.3f'% auc_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ver, el área debajo de la curva para el modelo RF es mejor que la LR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando trazo la curva ROC, me gusta agregar el AUC a la leyenda como se muestra a continuación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr_RF, tpr_RF,'r-',label = 'RF AUC: %.3f'%auc_RF)\n",
    "plt.plot(fpr_LR,tpr_LR,'b-', label= 'LR AUC: %.3f'%auc_LR)\n",
    "plt.plot([0,1],[0,1],'k-',label='random')\n",
    "plt.plot([0,0,1,1],[0,1,1,1],'g-',label='perfect')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el análisis predictivo, al decidir entre dos modelos es importante elegir la mejor  o mejores métrica de rendimiento en relación a la naturaleza del problema. Como puede ver aquí, hay muchos entre los que puede elegir (precisión, recuperación, precisión, puntaje f1, AUC, etc.). En última instancia, debe utilizar la métrica de rendimiento más adecuada para el problema comercial en cuestión. Muchos científicos de datos prefieren usar el AUC porque no requiere la selección de un umbral y ayuda a equilibrar la tasa positiva verdadera y la tasa falsa positiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EXTRA**\n",
    "**Curva ROC y curva presicion-recall (PR)**\n",
    "\n",
    "- Las curvas ROC resumen la compensación entre la tasa positiva verdadera y la tasa positiva falsa para un modelo predictivo que utiliza diferentes umbrales de probabilidad.\n",
    "- Las curvas de presicion-recall resumen la compensación entre la tasa positiva verdadera y el valor predictivo positivo para un modelo predictivo utilizando diferentes umbrales de probabilidad.\n",
    "- Las curvas ROC son apropiadas cuando las observaciones están equilibradas entre cada clase, mientras que las curvas de recuperación de precisión son apropiadas para conjuntos de datos desequilibrados.\n",
    "\n",
    "La curva PR es el resultado de dibujar la gráfica entre el precision y el recall. Esta gráfica nos permite ver a partir de qué recall tenemos una degradación de la precisión y viceversa. Lo ideal sería una curva que se acerque lo máximo posible a la esquina superior derecha (alta precisión y alto recall)\n",
    "\n",
    "![](img/pr.png)\n",
    "\n",
    "En en título del gráfico vemos AP=0.62. Este valor es el Average precision y es una manera de calcular el área bajo la curva PR o PR AUC, o lo que es lo mismo, el resultado de integrar la curva. El Average Precision nos sirve para evaluar y comparar el rendimiento de modelos. Cuanto más se acerque su valor a 1, mejor será nuestro modelo.\n",
    "\n",
    "**Diferencias entre curvas ROC y PR**\n",
    "\n",
    "Por lo general, usaremos la curva PR o el Average Precision cuando tengamos problemas de datasets no balanceados, es decir, cuando la clase positiva ocurre pocas veces. Cuando hay pocos ejemplos positivos, la curva ROC o el ROC AUC puede dar un valor alto, sin embargo, la curva PR estará lejos de su valor óptimo, poniendo de manifiesto un indicador de precisión relacionado con la baja probabilidad de la clase positiva.\n",
    "Será una opción interesante usar la curva ROC y el ROC AUC cuando tengamos un dataset más balanceado o queramos poner de manifiesto un indicador más relacionado con falsas alarmas (falsos positivos).\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1jyeHkbBzJwfC14TMuCaE_0QHKs7ofOV9\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconocimiento de imagen y CNNs\n",
    "\n",
    "Fuente: https://planetachatbot.com/c%C3%B3mo-empezar-con-reconocimiento-de-imagen-y-redes-neuronales-convolucionales-en-5-minutos-7f651054dfd7\n",
    "\n",
    "El reconocimiento de imagen es el problema de identificar y clasificar objetos en una imagen. Posiblemente, una de sus aplicaciones más comunes en la actualidad es el etiquetado automático de imágenes, usado para la gestión y organización de contenido web.\n",
    "\n",
    "Gracias a un tipo de modelos conocidos como Convolutional Neural Networks (CNNs), redes neuronales convolucionales, el reconocimiento de imagen ha experimentado enormes progresos. Estos modelos están inspirados por los procesos biológicos que tienen lugar en el cortex visual, donde las neuronas individuales responden a estímulos en un área restringida del campo visual. Este área se superpone parcialmente con el de las neuronas más proximas, cubriendo de forma colectiva el campo visual completo.\n",
    "\n",
    "Como resultado, las CNNs aprenden a responder a diferenctes caractéricas de la imagen (bordes, formas, etc), como los bancos de filtros utilizados en los algoritmos tradicionales y definidos de forma manual. De hecho, la capacidad de aprender dichos filtros supone una ventaja única de las CNNs, que elimina el esfuerzo manual requerido en el diseño de características.\n",
    "\n",
    "En la actualidad, existen multitud de arquitecturas de CNN disponibles de forma gratuita y sin restricciones de uso que pueden alcanzar un rendimiento razonable en tareas de reconocimiento visual. Por ejemplo, Keras — una librería de alto nivel que sirve como capa de abstracción sobre Tensorflow — proporciona acceso a alguno de los ganadores de la competición ImageNet ILSVRC. CNNs como ResNet50 (desarrollada por Microsoft Research) o InceptionV3 (desarrollada por Google Research) se ofrecen listas para reconocer 1000 objectos comunes (lista de objectos ILSVRC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando y ejecutando el modelo\n",
    "\n",
    "Cualquiera de los modelos de reconocimiento de imagen disponibles en Keras puede ser cargado con sólo dos líneas de código. Esto automáticamente descargará el modelo entrenado para clasificar 1000 objetos comunes usando el conjunto de datos de ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "model = InceptionV3(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El módulo keras.applications proporciona acceso a las arquitecturas más comunes, como VGG16, ResNet50, InceptionV3 o MobileNet. En este caso vamos a elegir la Inception-v3 de Google, que es una de las mejores, con un error top-5 del 3,46%. Valor que mide la frequencia con la que falla el modelo a la hora de predecir la respuesta correcta como una de sus cinco mejores estimaciones — error top-5 — ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de predicción\n",
    "\n",
    "Para empezar a hacer prediciones sólo necesitamos preparar la imagen de entrada y decodificar el vector de predición de salida. Para esto, vamos a escribir la función auxiliar predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "\n",
    "def predict(model, img_path, target_size=(299, 299), top_n=5):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    preds = model.predict(x)\n",
    "    return decode_predictions(preds, top=top_n)[0]https://ermlab.com/en/blog/nlp/cifar-10-classification-using-keras-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, la imagen tiene que ser redimensionada para adaptarse al tamaño de entrada de la red Inception-v3, en este caso target_size=(299, 299), otras redes como la VGG16 o la ResNet50 deben establecer un tamaño igual a (224, 224). La función image.load_img del módulo keras.preprocessing carga directamente la image desde su ubicación img_path y redimensiona la imagen al tamaño especificado target_size.\n",
    "\n",
    "El siguiente paso es convertir la imagen img a un array de numpy con image.img_to_array y cambiar sus dimensiones con np.expand_dim para pasar de (3, 299, 299) a (1, 3, 299, 299). De este modo se tiene un array de 4 dimensiones con 1 imagen de 3 canales RGB y tamaño 299 por 299. Esto es necesario porque la función model.predict requiere que la entrada sea un array de 4 dimensiones, pudiendo clasificar multiples imágenes de una sola vez.\n",
    "\n",
    "El último paso antes de la predicción es la normalización de los datos, que usa la función preprocess_input para centrarlos en cero usando la media de los valores de los canales de las imágenes del conjunto de entrenamiento. Este paso es extremadamente importante porque si no se realiza provocará que las probabilidades de las predicciones sean incorrectas.\n",
    "\n",
    "Finalmente, el valor devuelto por la función de predicción model.predict es decodificado por decode_predictions para devolver el resultado de la clasificiación a través de la correspondiente etiqueta y la probabilidad predicha.\n",
    "\n",
    "**Carga de la imagen**\n",
    "\n",
    "Definido nuestro sistema de reconocimiento, solo necesitamos cargar nuestras propias imágenes para probar su funcionamiento. Para ello, podemos usar directamente el código que podemos encontrar al hacer click en el botón negro que se encuentra en lo alto a la izquierda, justo debajo del menú."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_base = 'dog.jpg'\n",
    "base_image_path =  nombre_base\n",
    "fn=plt.imread(base_image_path)\n",
    "plt.imread(base_image_path)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.imshow(plt.imread(base_image_path))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_image(img_path):\n",
    "    img = image.load_img(fn, target_size=(299, 299))\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "def plot_pred(pred):\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    classes = [c[1] for c in pred]\n",
    "    probas = [c[2] for c in pred]\n",
    "    y_pos = np.arange(len(classes))\n",
    "    plt.barh(y_pos, probas, align='center')\n",
    "    plt.yticks(y_pos, classes)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Probability')\n",
    "    plt.xlim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicción**\n",
    "\n",
    "Cargada la imagen, obtendremos una predición que se mostrará como a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(model, nombre_base)\n",
    "plot_pred(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CIFAR10 Dataset\n",
    "\n",
    "Fuente: Khipu 2019\n",
    "\n",
    "Ahora que entendemos las capas convolucionales, podemos combinarlas como bloques de construcción para construir un clasificador ConvNet para imágenes. Para esta práctica, utilizaremos el conjunto de datos de imagen en color CIFAR10 (pronunciado \"seefar ten\") que consta de 50,000 imágenes de entrenamiento y 10,000 imágenes de prueba. Tomamos 10,000 imágenes del conjunto de entrenamiento para formar un conjunto de validación y visualizar algunas imágenes de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(train_images, train_labels), (test_images, test_labels)= cifar10.load_data()\n",
    "cifar_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the last 10000 images from the training set to form a validation set\n",
    "train_labels = train_labels.squeeze()\n",
    "validation_images = train_images[-10000:, :, :]\n",
    "validation_labels = train_labels[-10000:]\n",
    "train_images = train_images[:-10000, :, :]\n",
    "train_labels = train_labels[:-10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles son las formas y los tipos de datos de train_images y train_labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_images.shape = {}, data-type = {}'.format(train_images.shape, train_images.dtype))\n",
    "print('train_labels.shape = {}, data-type = {}'.format(train_labels.shape, train_labels.dtype))\n",
    "\n",
    "print('validation_images.shape = {}, data-type = {}'.format(validation_images.shape, validation_images.dtype))\n",
    "print('validation_labels.shape = {}, data-type = {}'.format(validation_labels.shape, validation_labels.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar ejemplos del conjunto de datos.\n",
    "Ejecute la celda a continuación varias veces para ver varias imágenes. (Pueden verse un poco borrosos porque hemos eliminado las imágenes pequeñas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid('off')\n",
    "\n",
    "  img_index = np.random.randint(0, 40000)\n",
    "  plt.imshow(train_images[img_index])\n",
    "  plt.xlabel(cifar_labels[train_labels[img_index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A ConvNet Classifier\n",
    "\n",
    "Finalmente, construimos una arquitectura convolucional simple para clasificar las imágenes CIFAR. Construiremos una versión mini de la arquitectura AlexNet, que consta de 5 capas convolucionales con agrupación máxima, seguidas de 3 capas completamente conectadas al final. Para investigar el efecto que tiene cada una de estas dos capas en la cantidad de parámetros, construiremos el modelo en dos etapas.\n",
    "\n",
    "Primero, las capas convolucionales + max-pooling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the convolutinal part of the model architecture using Keras Layers.\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=48, kernel_size=(3, 3), activation=tf.nn.relu, input_shape=(32, 32, 3), padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(3, 3)),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=tf.nn.relu, padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(3, 3)),\n",
    "    tf.keras.layers.Conv2D(filters=192, kernel_size=(3, 3), activation=tf.nn.relu, padding='same'),\n",
    "    tf.keras.layers.Conv2D(filters=192, kernel_size=(3, 3), activation=tf.nn.relu, padding='same'),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=tf.nn.relu, padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(3, 3)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuántos parámetros hay en la parte convolucional de la arquitectura? Podemos inspeccionar esto fácilmente usando la función de resumen del modelo en Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora agregamos una parte completamente conectada. Tenga en cuenta que también agregamos \"Dropout\" después de la primera capa completamente conectada. Dropout es una técnica de regularización que elimina de forma aleatoria las conexiones (\"caídas\") entre las neuronas, y fue una de las innovaciones clave del documento de AlexNet en 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten())  # Flatten \"squeezes\" a 3-D volume down into a single vector.\n",
    "model.add(tf.keras.layers.Dense(1024, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "model.add(tf.keras.layers.Dense(1024, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura adicional opcional: esquemas de inicialización aleatoria\n",
    "Es posible que se haya preguntado qué valores estamos usando para los valores iniciales de los pesos y sesgos en nuestro modelo. La respuesta corta es que generalmente usamos inicialización aleatoria. En este caso, acabamos de utilizar los inicializadores Keras predeterminados para cada capa, que generalmente son suficientes.\n",
    "\n",
    "La respuesta más larga es que el uso de números completamente aleatorios no siempre funciona mejor en la práctica y que hay una serie de esquemas de inicialización comunes (que están disponibles en la mayoría de los marcos de aprendizaje profundo como TensorFlow y Keras).\n",
    "\n",
    "Consideremos algunos ejemplos:\n",
    "\n",
    " * Cuando se usa la activación de ReLU, es común inicializar los sesgos con pequeños números positivos porque esto alienta a las activaciones de ReLU a comenzar en el estado _on_.\n",
    "\n",
    " * Cuanto más profundas sean las redes neuronales, más probable es que los gradientes se reduzcan hasta el punto en que desaparezcan o crezcan hasta el punto en que se desborden (los problemas de gradientes que desaparecen y explotan). Para ayudar a combatir esto, podemos inicializar nuestros pesos para tener una escala apropiada (específica del modelo). Un método para hacerlo se llama inicialización [Xavier o Glorot] (http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).\n",
    "\n",
    " * El esquema de inicialización _Xavier_ fue diseñado con las activaciones tradicionales Sigmoid y TanH y no funciona tan bien para las activaciones ReLU. Una alternativa es la inicialización [Él o Kaiming] (https://arxiv.org/pdf/1502.01852.pdf), que es una modificación de la inicialización de Xavier para las activaciones de ReLU.\n",
    "\n",
    " [Este blog] (http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization) entra en más detalles sobre la inicialización _He_ y _Xavier_. [La documentación de Keras] (https://keras.io/initializers/) enumera una serie de esquemas comunes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyamos un diagrama de flujo del modelo que hemos construido para ver cómo fluye la información entre las diferentes capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='small_lenet.png', show_shapes=True, show_layer_names=True)\n",
    "display.display(display.Image('small_lenet.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formación y validación del modelo.\n",
    "\n",
    "En la última práctica, escribimos la tubería del conjunto de datos, la función de pérdida y el ciclo de entrenamiento para darle una buena apreciación de cómo funciona. Esta vez, usamos el bucle de entrenamiento integrado en Keras. Para conjuntos de datos simples y estándar como CIFAR, hacerlo de esta manera funcionará bien, ¡pero es importante saber qué sucede debajo del capó porque es posible que deba escribir algunos o todos los pasos manualmente cuando trabaje con conjuntos de datos más complejos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 10  # The number of epochs (full passes through the data) to train for\n",
    "\n",
    "# Compiling the model adds a loss function, optimiser and metrics to track during training\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# The fit function allows you to fit the compiled model to some training data\n",
    "model.fit(x=train_images,\n",
    "          y=train_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epochs,\n",
    "          validation_data=(validation_images, validation_labels.astype(np.float32)))\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del Test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values = model.evaluate(x=test_images, y=test_labels)\n",
    "\n",
    "print('Final TEST performance')\n",
    "for metric_value, metric_name in zip(metric_values, model.metrics_names):\n",
    "  print('{}: {}'.format(metric_name, metric_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenga en cuenta que logramos aproximadamente un 80% de precisión del conjunto de entrenamiento, pero nuestra precisión de prueba es solo de alrededor del 67%. ¿Cuál crees que puede ser la razón de esto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificando ejemplos\n",
    "Ahora usamos nuestro modelo entrenado para clasificar una muestra de 25 imágenes del conjunto de prueba. Pasamos estas 25 imágenes a la función `` model.predict```, que devuelve una matriz dimensional [25, 10]. La entrada en la posición $ (i, j) $ de esta matriz contiene la probabilidad de que la imagen $ i $ pertenezca a la clase $ j $. Obtenemos la predicción más probable utilizando la función `` np.argmax '' que devuelve el índice de la entrada máxima a lo largo de las columnas. Finalmente, graficamos el resultado con la predicción y la probabilidad de predicción etiquetadas debajo de la imagen y la etiqueta verdadera en el lateral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_indices = np.random.randint(0, len(test_images), size=[25])\n",
    "sample_test_images = test_images[img_indices]\n",
    "sample_test_labels = [cifar_labels[i] for i in test_labels[img_indices].squeeze()]\n",
    "\n",
    "predictions = model.predict(sample_test_images)\n",
    "max_prediction = np.argmax(predictions, axis=1)\n",
    "prediction_probs = np.max(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i, (img, prediction, prob, true_label) in enumerate(\n",
    "    zip(sample_test_images, max_prediction, prediction_probs, sample_test_labels)):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid('off')\n",
    "\n",
    "  plt.imshow(img)\n",
    "  plt.xlabel('{} ({:0.3f})'.format(cifar_labels[prediction], prob))\n",
    "  plt.ylabel('{}'.format(true_label))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
